package ai

import (
	"encoding/json"
	"net/http"

	http_internal "runtime.link/api/internal/http"
	"runtime.link/api/unix"
	"runtime.link/xyz"
)

type Error struct {
	Code ErrorCode `json:"code"`
}

func (e Error) Error() string {
	return e.Code.String()
}

type ErrorCode xyz.Switch[string, struct {
	ContextLengthExceeded ErrorCode `json:"context_length_exceeded"`
	ModelNotFound         ErrorCode `json:"model_not_found"`
}]

var ErrorCodes = xyz.AccessorFor(ErrorCode.Values)

// Client authentication.
func Client(key string) *http.Client {
	return http_internal.ClientWithHeader("Authorization", "Bearer "+key)
}

type ChatCompletionRequest struct {
	Model Model `json:"model"
		model to use.`
	Messages []Message `json:"messages"
		comprising the conversation so far.`
	Functions []Function `json:"functions,omitempty"
		the model may generate JSON inputs for.`
	FunctionCall string `json:"function_call,omitempty"
		controls how the model responds to function calls. "none" means the model does not
		call a function, and responds to the end-user. "auto" means the model can pick
		between an end-user or calling a function. Specifying a particular function via
		{"name":\ "my_function"} forces the model to call that function. "none" is the
		default when no functions are present. "auto" is the default if functions are present.`
	Temperature *float64 `json:"temperature,omitempty"
		to use, between 0 and 2. Higher values like 0.8 will make the output more random,
		while lower values like 0.2 will make it more focused and deterministic.

		We generally recommend altering this or top_p but not both.`
	TopP *float64 `json:"top_p,omitempty"
		An alternative to sampling with temperature, called nucleus sampling, where the model
		considers the results of the tokens with top_p probability mass. So 0.1 means only
		the tokens comprising the top 10% probability mass are considered.

		We generally recommend altering this or temperature but not both.`
	N uint `json:"n,omitempty"
		many chat completion choices to generate for each input message.`
	Stop StringOrSlice `json:"stop,omitempty"
		up to 4 sequences where the API will stop generating further tokens.`
	MaxTokens uint `json:"max_tokens,omitempty"
		to generate in the chat completion.

		The total length of input tokens and generated tokens is limited
		by the model's context length.`
	PresencePenalty float64 `json:"presence_penalty,omitempty"
		number between -2.0 and 2.0. Positive values penalize new tokens based
		on whether they appear in the text so far, increasing the model's
		likelihood to talk about new topics.`
	FrequencyPenalty float64 `json:"frequency_penalty,omitempty"
		number between -2.0 and 2.0. Positive values penalize new tokens based on
		their existing frequency in the text so far, decreasing the model's
		likelihood to repeat the same line verbatim.`
	LogitBias map[Token]float64 `json:"logit_bias,omitempty"
		modifies the likelihood of specified tokens appearing in the completion.

		Accepts a json object that maps tokens (specified by their token ID in the tokenizer)
		to an associated bias value from -100 to 100. Mathematically, the bias is added to the
		logits generated by the model prior to sampling. The exact effect will vary per model,
		but values between -1 and 1 should decrease or increase likelihood of selection; values
		like -100 or 100 should result in a ban or exclusive selection of the relevant token.`
	User string `json:"user,omitempty"
		is a unique identifier representing your end-user, which can help OpenAI to monitor and
		detect abuse.`
}

type Token string

type StringOrSlice []string

func (s StringOrSlice) MarshalJSON() ([]byte, error) {
	switch len(s) {
	case 0:
		return []byte("null"), nil
	case 1:
		return json.Marshal(s[0])
	default:
		return json.Marshal([]string(s))
	}
}

func (s *StringOrSlice) UnmarshalJSON(body []byte) error {
	if len(body) == 0 {
		return nil
	}
	switch body[0] {
	case '"':
		var str string
		if err := json.Unmarshal(body, &str); err != nil {
			return err
		}
		*s = []string{str}
		return nil
	case '[':
		var arr []string
		if err := json.Unmarshal(body, &arr); err != nil {
			return err
		}
		*s = arr
		return nil
	default:
		return json.Unmarshal(body, (*[]string)(s))
	}
}

type StreamedChatCompletionRequest struct {
	ChatCompletionRequest

	Stream TRUE `json:"stream"`
}

type TRUE xyz.Static[TRUE, bool]

func (t TRUE) Value() bool { return true }

type Model string

type ChatCompletion struct {
	ID string `json:"id"
		is a unique identifier for the chat completion.`
	Object  isChatCompletion `json:"object"`
	Created unix.Seconds     `json:"created"
		time when the chat completion was created.`
	Model Model `json:"model"
		used for the chat completion.`
	Choices []Choice `json:"choices"
		list of chat completion choices. Can be more than one if n is greater than 1.`
	Usage Usage `json:"usage"
		statistics for the completion request.`
}

type isChatCompletion xyz.Static[isChatCompletion, string]

func (isChatCompletion) Value() string { return "chat.completion" }

type ChatCompletionChunk struct {
	ID string `json:"id"
		is a unique identifier for the chat completion.`
	Object  isChatCompletionChunk `json:"object"`
	Created unix.Seconds          `json:"created"
		time when the chat completion was created.`
	Model Model `json:"model"
		used for the chat completion.`
	Choices []ChoiceChunk `json:"choices"
		list of chat completion choices. Can be more than one if n is greater than 1.`
}

type isChatCompletionChunk xyz.Static[isChatCompletionChunk, string]

func (isChatCompletionChunk) Value() string { return "chat.completion.chunk" }

type FinishReason xyz.Switch[string, struct {
	Stop FinishReason `json:"stop"
		the model hit a natural stop point or a provided stop sequence.`
	Length FinishReason `json:"length"
		the maximum number of tokens specified in the request was reached`
	FunctionCall FinishReason `json:"function_call"
		if the model called a function.`
}]

var FinishReasons = xyz.AccessorFor(FinishReason.Values)

type Choice struct {
	Index int `json:"index"
		of the choice in the list of choices.`
	Message Message `json:"message"
		generated by the model.`
	FinishReason FinishReason `json:"finish_reason"
		why the model stopped generating tokens.`
}

type ChoiceChunk struct {
	Index int `json:"index"
		of the choice in the list of choices.`
	Delta Message `json:"delta"
		generated by streamed model responses.`
	FinishReason FinishReason `json:"finish_reason"
		why the model stopped generating tokens.`
}

type Role xyz.Switch[string, struct {
	System    Role `json:"system"`
	Assistant Role `json:"assistant"`
	User      Role `json:"user"`
	Function  Role `json:"function"`
}]

var Roles = xyz.AccessorFor(Role.Values)

type Message struct {
	Name string `json:"name,omitempty"
		of the author of this message. name is required if role is function, and it
		should be the name of the function whose response is in the content. May
		contain a-z, A-Z, 0-9, and underscores, with a maximum length of 64 characters.`
	Role Role `json:"role"
		of the author of this message.`
	Content string `json:"content"
		of the message.`
	FunctionCall *FunctionCall `json:"function_call,omitempty"
		name and arguments of a function that should be called, as generated by the model.`
}

type FunctionCall struct {
	Name string `json:"name"
		of the function to call.`
	Arguments string `json:"arguments"
		to call the function with, as generated by the model in JSON format.
		Note that the model does not always generate valid JSON, and may
		hallucinate parameters not defined by your function schema. Validate
		the arguments in your code before calling your function.`
}

type Usage struct {
	PromptTokens int `json:"prompt_tokens"
		is the number of tokens in the prompt.`
	CompletionTokens int `json:"completion_tokens"
		is the number of tokens in the generated completion.`
	TotalTokens int `json:"total_tokens"
		is the total number of tokens used in the request (prompt + completion).`
}

type Function struct {
	Name string `json:"name"
		of the function to be called. Must be a-z, A-Z, 0-9,
		or contain underscores and dashes, with a maximum length of 64.`
	Description string `json:"description,omitempty"
		of what the function does, used by the model to choose when and how
		to call the function.`
	Parameters json.RawMessage `json:"parameters,omitempty"
		the functions accepts, described as a JSON Schema object.`
}
