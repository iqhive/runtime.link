package ai

import (
	"runtime.link/api/unix"
	"runtime.link/xyz"
)

type ChatCompletionRequest struct {
	Messages []Message `json:"messages"
		comprising the conversation so far.`
	Model Model `json:"model"
		model to use.`
	FrequencyPenalty float64 `json:"frequency_penalty,omitempty"
		number between -2.0 and 2.0. Positive values penalize new tokens based on
		their existing frequency in the text so far, decreasing the model's
		likelihood to repeat the same line verbatim.`
	MaxTokens uint `json:"max_tokens,omitempty"
		to generate in the chat completion.

		The total length of input tokens and generated tokens is limited
		by the model's context length.`
	PresencePenalty float64 `json:"presence_penalty,omitempty"
		number between -2.0 and 2.0. Positive values penalize new tokens based
		on whether they appear in the text so far, increasing the model's
		likelihood to talk about new topics.`
	ResponseFormat ResponseFormat `json:"response_format,omitzero"
		An object specifying the format that the model must output.`
	Stop []string `json:"stop,omitempty"
		up to 4 sequences where the API will stop generating further tokens.`
	Temperature *float64 `json:"temperature,omitempty"
		to use, between 0 and 2. Higher values like 0.8 will make the output more random,
		while lower values like 0.2 will make it more focused and deterministic.

		We generally recommend altering this or top_p but not both.`
	TopP *float64 `json:"top_p,omitempty"
		An alternative to sampling with temperature, called nucleus sampling, where the model
		considers the results of the tokens with top_p probability mass. So 0.1 means only
		the tokens comprising the top 10% probability mass are considered.

		We generally recommend altering this or temperature but not both.`
	Tools []Tool `json:"tools,omitempty"
		the model may call.`
	ToolChoice ToolChoice `json:"tool_choice,omitzero"
		controls which (if any) tool is called by the model.`
	LogProbs bool `json:"logprobs,omitempty"
		Whether to return log probabilities of the output tokens or not. If true, returns the log
		probabilities of each output token returned in the content of message.`
	TopLogProbs uint `json:"top_logprobs,omitempty"
		An integer between 0 and 20 specifying the number of most likely tokens to return at each
		token position, each with an associated log probability. LogProbs must be set to true if
		this parameter is used.`
}

type Token string

type StreamedChatCompletionRequest struct {
	ChatCompletionRequest

	Stream isTrue `json:"stream"`
}

type isTrue xyz.Static[isTrue, bool]

func (t isTrue) Value() bool { return true }

type Model string

type ChatCompletion struct {
	ID string `json:"id"
		is a unique identifier for the chat completion.`
	Object  isChatCompletion `json:"object"`
	Created unix.Seconds     `json:"created"
		time when the chat completion was created.`
	Model Model `json:"model"
		used for the chat completion.`
	Choices []Choice `json:"choices"
		list of chat completion choices. Can be more than one if n is greater than 1.`
	Usage Usage `json:"usage"
		statistics for the completion request.`
	SystemFingerprint string `json:"system_fingerprint,omitempty"
		represents the backend configuration that the model runs with.`
}

type isChatCompletion xyz.Static[isChatCompletion, string]

func (isChatCompletion) Value() string { return "chat.completion" }

type ChatCompletionChunk struct {
	ID string `json:"id"
		is a unique identifier for the chat completion.`
	Object  isChatCompletionChunk `json:"object"`
	Created unix.Seconds          `json:"created"
		time when the chat completion was created.`
	Model Model `json:"model"
		used for the chat completion.`
	Choices []ChoiceChunk `json:"choices"
		list of chat completion choices. Can be more than one if n is greater than 1.`
}

type isChatCompletionChunk xyz.Static[isChatCompletionChunk, string]

func (isChatCompletionChunk) Value() string { return "chat.completion.chunk" }

type FinishReason xyz.Switch[string, struct {
	Stop FinishReason `json:"stop"
		the model hit a natural stop point or a provided stop sequence.`
	Length FinishReason `json:"length"
		the maximum number of tokens specified in the request was reached`
	ContentFilter FinishReason `json:"content_filter"
		the model's content filter stopped the model from generating more tokens.`
	ToolCalls FinishReason `json:"tool_calls"
		the model's tool calls stopped the model from generating more tokens.`
}]

var FinishReasons = xyz.AccessorFor(FinishReason.Values)

type Choice struct {
	Index int `json:"index"
		of the choice in the list of choices.`
	Message Message `json:"message"
		generated by the model.`
	LoggedProbabilities []LoggedProbability `json:"logprobs,omitempty"
		of the generated tokens.`
	FinishReason FinishReason `json:"finish_reason"
		why the model stopped generating tokens.`
}

type ChoiceChunk struct {
	Index int `json:"index"
		of the choice in the list of choices.`
	Delta Message `json:"delta"
		generated by streamed model responses.`
	FinishReason FinishReason `json:"finish_reason"
		why the model stopped generating tokens.`
}

type Role xyz.Switch[string, struct {
	System    Role `json:"system"`
	Assistant Role `json:"assistant"`
	User      Role `json:"user"`
	Function  Role `json:"function"`
}]

var Roles = xyz.AccessorFor(Role.Values)

type Message xyz.Tagged[any, struct {
	System    xyz.Case[Message, System]     `json:"?role=system"`
	User      xyz.Case[Message, User]       `json:"?role=user"`
	Assistant xyz.Case[Message, Assistant]  `json:"?role=assistant"`
	Tool      xyz.Case[Message, ToolOutput] `json:"?role=tool"`
}]

type System struct {
	Name string `json:"name,omitempty"
		of the system.`
	Content string `json:"content"
		of the message.`
}

type User struct {
	Name string `json:"name,omitempty"
		of the system.`
	Content string `json:"content"
		of the message.`
}

type Assistant struct {
	Name string `json:"name,omitempty"
		of the system.`
	Content string `json:"content"
		of the message.`
	ToolCalls []Tool `json:"tool_calls,omitempty"
		made by the assistant.`
}

type ToolCallID string

type FunctionCall struct {
	ID       ToolCallID `json:"id"`
	Function Function   `json:"function"`
}

type ToolOutput struct {
	ToolCallID ToolCallID `json:"tool_call_id,omitempty"
		identifies the tool call.`
	Content string `json:"content"
		of the message.`
}

type Usage struct {
	PromptTokens int `json:"prompt_tokens"
		is the number of tokens in the prompt.`
	CompletionTokens int `json:"completion_tokens"
		is the number of tokens in the generated completion.`
	TotalTokens int `json:"total_tokens"
		is the total number of tokens used in the request (prompt + completion).`
	PromptTokensDetails PromptTokensDetails `json:"prompt_tokens_details"`
}

type PromptTokensDetails struct {
	TextTokens   int `json:"text_tokens"`
	AudioTokens  int `json:"audio_tokens"`
	ImageTokens  int `json:"image_tokens"`
	CachedTokens int `json:"cached_tokens"`
}

type ResponseFormat xyz.Tagged[any, struct {
	JSON ResponseFormat `json:"?type=json_object"`
}]

var ResponseFormats = xyz.AccessorFor(ResponseFormat.Values)

type Tool xyz.Tagged[any, struct {
	Function Function `json:"function?type=function"`
}]

var Tools = xyz.AccessorFor(Tool.Values)

type Function struct {
	Name string `json:"name"
		of the function to call.`
	Description string `json:"description,omitempty"
		of what the function does, used by the model to choose when and how to call the function.`
	Arguments string `json:"arguments,omitempty"
		(for function calls) to call the function with.`
}

type ToolChoice xyz.Tagged[any, struct {
	None     ToolChoice                     `json:"none"`
	Auto     ToolChoice                     `json:"auto"`
	Required ToolChoice                     `json:"required"`
	Specific xyz.Case[ToolChoice, Function] `json:"function?type=function"`
}]

var ToolChoices = xyz.AccessorFor(ToolChoice.Values)

type LoggedProbability struct {
	Content []TokenProbability `json:"content"`
	Refusal []TokenProbability `json:"refusal,omitempty"`
}

type TokenProbability struct {
	Bytes            []rune             `json:"bytes"`
	Probability      float64            `json:"probability"`
	Token            string             `json:"token"`
	TopProbabilities []TokenProbability `json:"top_logprobs,omitempty"`
}
